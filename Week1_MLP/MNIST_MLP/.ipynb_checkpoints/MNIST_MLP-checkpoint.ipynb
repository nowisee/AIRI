{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import scipy.misc\n",
    "import sys\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "imageNum = 60000\n",
    "testImageNum = 10000\n",
    "typesOfNumber = 10\n",
    "\n",
    "row = 28\n",
    "col = 28\n",
    "\n",
    "batchSize = 256\n",
    "batchCount = imageNum // batchSize\n",
    "\n",
    "hidden1NodeNum = 300\n",
    "hidden2NodeNum = 200\n",
    "hidden3NodeNum = 100\n",
    "\n",
    "dropoutRate = 0.5\n",
    "\n",
    "learningRate = 0.005\n",
    "iteration = 100\n",
    "\n",
    "##\n",
    "X = tf.placeholder(shape=[None, row*col], dtype=tf.float32, name='X')\n",
    "Y = tf.placeholder(shape=[None], dtype=tf.int64, name='Y')\n",
    "Y_onehot = tf.one_hot(Y, typesOfNumber, axis=1)\n",
    "\n",
    "TRAIN = tf.placeholder(shape=None, dtype=tf.bool, name='TRAIN')\n",
    "\n",
    "W1 = tf.Variable(tf.truncated_normal([row*col, hidden1NodeNum], stddev=0.02, name='weights1'))\n",
    "B1 = tf.Variable(tf.zeros([hidden1NodeNum]), name='biases1')\n",
    "hidden1 = tf.matmul(X, W1) + B1\n",
    "hidden1 = tf.nn.relu(hidden1, \"relu1\")\n",
    "hidden1 = tf.layers.dropout(hidden1, dropoutRate, TRAIN, \"dropout1\")\n",
    "\n",
    "W2 = tf.Variable(tf.truncated_normal([hidden1NodeNum, hidden2NodeNum], stddev=0.02, name='weights2'))\n",
    "B2 = tf.Variable(tf.zeros([hidden2NodeNum]), name='biases2')\n",
    "hidden2 = tf.matmul(hidden1, W2) + B2\n",
    "hidden2 = tf.nn.relu(hidden2, \"relu2\")\n",
    "hidden2 = tf.layers.dropout(hidden2, dropoutRate, TRAIN, \"dropout2\")\n",
    "\n",
    "W3 = tf.Variable(tf.truncated_normal([hidden2NodeNum, hidden3NodeNum], stddev=0.02, name='weights3'))\n",
    "B3 = tf.Variable(tf.zeros([hidden3NodeNum]), name='biases3')\n",
    "hidden3 = tf.matmul(hidden2, W3) + B3\n",
    "hidden3 = tf.nn.relu(hidden3, \"relu3\")\n",
    "hidden3 = tf.layers.dropout(hidden3, dropoutRate, TRAIN, \"dropout3\")\n",
    "\n",
    "W4 = tf.Variable(tf.zeros([hidden3NodeNum, typesOfNumber], name='weights4'))\n",
    "B4 = tf.Variable(tf.zeros([typesOfNumber]), name='biases4')\n",
    "Y_pred = tf.matmul(hidden3, W4) + B4\n",
    "Y_pred = tf.layers.dropout(Y_pred, dropoutRate, TRAIN, \"dropout3\")\n",
    "Y_pred_softmax = tf.nn.softmax(Y_pred)\n",
    "\n",
    "with tf.name_scope(\"Reduce_mean_with_softmax_cross_entropy_with_logits\"):\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=Y_pred, labels=Y_onehot))\n",
    "\n",
    "##\n",
    "trainer = tf.train.GradientDescentOptimizer(learningRate)\n",
    "optimizer = trainer.minimize(loss)\n",
    "\n",
    "correct = tf.equal(tf.argmax(Y_pred_softmax, 1), tf.argmax(Y_onehot, 1))\n",
    "acc = tf.reduce_mean(tf.cast(correct, tf.float32))    \n",
    "\n",
    "saver = tf.train.Saver()\n",
    "path = './models/simple_nn'\n",
    "\n",
    "writer = tf.summary.FileWriter('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch_0001: 538.656714 , acc =  0.112367\n",
      "epoch_0002: 538.350288 , acc =  0.112367\n",
      "epoch_0003: 538.005929 , acc =  0.1134\n",
      "epoch_0004: 537.573600 , acc =  0.12865\n",
      "epoch_0005: 536.978928 , acc =  0.1718\n",
      "epoch_0006: 536.099640 , acc =  0.225883\n",
      "epoch_0007: 534.716614 , acc =  0.259283\n",
      "epoch_0008: 532.409446 , acc =  0.263467\n",
      "epoch_0009: 528.345822 , acc =  0.255267\n",
      "epoch_0010: 520.961653 , acc =  0.256283\n",
      "epoch_0011: 508.007380 , acc =  0.285683\n",
      "epoch_0012: 487.522451 , acc =  0.353417\n",
      "epoch_0013: 456.616187 , acc =  0.438867\n",
      "epoch_0014: 410.726575 , acc =  0.535033\n",
      "epoch_0015: 355.908791 , acc =  0.609283\n",
      "epoch_0016: 305.992445 , acc =  0.657684\n",
      "epoch_0017: 264.168797 , acc =  0.6966\n",
      "epoch_0018: 230.750641 , acc =  0.72815\n",
      "epoch_0019: 205.858921 , acc =  0.751834\n",
      "epoch_0020: 187.644726 , acc =  0.771101\n",
      "epoch_0021: 173.855709 , acc =  0.787901\n",
      "epoch_0022: 162.846894 , acc =  0.801667\n",
      "epoch_0023: 153.640445 , acc =  0.813367\n",
      "epoch_0024: 145.715704 , acc =  0.823767\n",
      "epoch_0025: 138.817916 , acc =  0.832401\n",
      "epoch_0026: 132.813571 , acc =  0.839451\n",
      "epoch_0027: 127.594671 , acc =  0.846184\n",
      "epoch_0028: 123.050586 , acc =  0.852134\n",
      "epoch_0029: 119.067564 , acc =  0.857034\n",
      "epoch_0030: 115.542780 , acc =  0.860668\n",
      "epoch_0031: 112.393822 , acc =  0.864718\n",
      "epoch_0032: 109.555650 , acc =  0.868284\n",
      "epoch_0033: 106.979891 , acc =  0.870968\n",
      "epoch_0034: 104.631307 , acc =  0.873768\n",
      "epoch_0035: 102.479334 , acc =  0.876401\n",
      "epoch_0036: 100.500751 , acc =  0.878818\n",
      "epoch_0037: 98.676071 , acc =  0.880918\n",
      "epoch_0038: 96.988097 , acc =  0.883484\n",
      "epoch_0039: 95.422773 , acc =  0.885368\n",
      "epoch_0040: 93.968169 , acc =  0.887101\n",
      "epoch_0041: 92.613060 , acc =  0.888684\n",
      "epoch_0042: 91.347744 , acc =  0.890218\n",
      "epoch_0043: 90.163078 , acc =  0.891651\n",
      "epoch_0044: 89.048879 , acc =  0.892618\n",
      "epoch_0045: 87.998246 , acc =  0.893851\n",
      "epoch_0046: 87.004297 , acc =  0.895068\n",
      "epoch_0047: 86.062009 , acc =  0.896284\n",
      "epoch_0048: 85.164809 , acc =  0.897484\n",
      "epoch_0049: 84.305382 , acc =  0.898151\n",
      "epoch_0050: 83.481131 , acc =  0.899018\n",
      "epoch_0051: 82.686729 , acc =  0.900034\n",
      "epoch_0052: 81.918896 , acc =  0.901084\n",
      "epoch_0053: 81.173894 , acc =  0.901868\n",
      "epoch_0054: 80.450455 , acc =  0.902868\n",
      "epoch_0055: 79.747316 , acc =  0.903501\n",
      "epoch_0056: 79.063033 , acc =  0.904068\n",
      "epoch_0057: 78.396047 , acc =  0.904968\n",
      "epoch_0058: 77.744014 , acc =  0.905718\n",
      "epoch_0059: 77.105106 , acc =  0.906584\n",
      "epoch_0060: 76.478223 , acc =  0.907134\n",
      "epoch_0061: 75.864153 , acc =  0.907851\n",
      "epoch_0062: 75.261187 , acc =  0.908751\n",
      "epoch_0063: 74.667333 , acc =  0.909418\n",
      "epoch_0064: 74.083294 , acc =  0.910384\n",
      "epoch_0065: 73.507549 , acc =  0.911034\n",
      "epoch_0066: 72.940755 , acc =  0.911768\n",
      "epoch_0067: 72.381653 , acc =  0.912584\n",
      "epoch_0068: 71.829651 , acc =  0.913351\n",
      "epoch_0069: 71.284375 , acc =  0.914118\n",
      "epoch_0070: 70.745156 , acc =  0.915001\n",
      "epoch_0071: 70.211270 , acc =  0.915634\n",
      "epoch_0072: 69.681588 , acc =  0.916251\n",
      "epoch_0073: 69.155748 , acc =  0.916968\n",
      "epoch_0074: 68.633588 , acc =  0.917568\n",
      "epoch_0075: 68.114202 , acc =  0.918234\n",
      "epoch_0076: 67.598576 , acc =  0.918834\n",
      "epoch_0077: 67.086617 , acc =  0.919534\n",
      "epoch_0078: 66.578994 , acc =  0.919934\n",
      "epoch_0079: 66.073474 , acc =  0.920551\n",
      "epoch_0080: 65.569491 , acc =  0.921084\n",
      "epoch_0081: 65.067237 , acc =  0.921851\n",
      "epoch_0082: 64.567925 , acc =  0.922368\n",
      "epoch_0083: 64.070647 , acc =  0.922918\n",
      "epoch_0084: 63.576187 , acc =  0.923568\n",
      "epoch_0085: 63.083949 , acc =  0.924084\n"
     ]
    }
   ],
   "source": [
    "if sys.argv[1] == 'train':\n",
    "    ##\n",
    "    fileobj = open('train-images.idx3-ubyte')\n",
    "    images = np.fromfile(file=fileobj, dtype=np.uint8)\n",
    "    images = images[16:].reshape([imageNum, row*col]).astype(np.float)\n",
    "\n",
    "    fileobj = open('train-labels.idx1-ubyte')\n",
    "    labels = np.fromfile(file=fileobj, dtype=np.uint8)\n",
    "    labels = labels[8:].reshape([imageNum]).astype(np.int)\n",
    "    \n",
    "    ##\n",
    "    images = images / 255.0\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        tf.summary.FileWriter('./graphs', sess.graph)\n",
    "\n",
    "        for epoch in range(iteration):\n",
    "            total_loss = 0  \n",
    "\n",
    "            for i in range(batchCount):\n",
    "                batchIndex = i * batchSize\n",
    "                img = np.reshape(images[batchIndex:batchIndex+batchSize], [batchSize, row*col])\n",
    "                label = labels[batchIndex:batchIndex+batchSize]\n",
    "                _, loss_v = sess.run([optimizer, loss], feed_dict={X:img, Y:label, TRAIN:True})\n",
    "                total_loss += loss_v\n",
    "\n",
    "            saver.save(sess, path)\n",
    "            print 'epoch_%04d: %.6f' %(epoch+1, total_loss), ', acc = ', acc.eval(session=sess, feed_dict={X:images, Y:labels})\n",
    "            \n",
    "        #print'Test image acc = ', acc.eval(session=sess, feed_dict={X:test_images, Y:test_labels})\n",
    "\n",
    "        #trained_W1, trained_W2, trained_W3 = sess.run([W1, W2, W3])\n",
    "    \n",
    "    ## Weight Value Visualization\n",
    "#     trained_weight = np.transpose(trained_weight)\n",
    "#     weight = (trained_weight).reshape([typesOfNumber, row, col]).astype(np.float32)\n",
    "\n",
    "#     for i in range(typesOfNumber):\n",
    "#         maxVal = np.max(weight[i])\n",
    "#         minVal = np.min(weight[i])\n",
    "#         weight[i] = 255 * (weight[i] - minVal) / (maxVal-minVal)\n",
    "#         name = 'weight' + str(i) + '.jpg'\n",
    "#         scipy.misc.imsave(name, weight[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if sys.argv[1] == 'test':\n",
    "    if sys.argv[2] == 'MNIST_data':\n",
    "        fileobj = open('t10k-images.idx3-ubyte')\n",
    "        test_images = np.fromfile(file=fileobj, dtype=np.uint8)\n",
    "        test_images = test_images[16:].reshape([testImageNum, row*col]).astype(np.float)\n",
    "\n",
    "        fileobj = open('t10k-labels.idx1-ubyte')\n",
    "        test_labels = np.fromfile(file=fileobj, dtype=np.uint8)\n",
    "        test_labels = test_labels[8:].reshape([testImageNum]).astype(np.int)\n",
    "        \n",
    "        test_images = test_images / 255.0\n",
    "\n",
    "        with tf.Session() as sess:\n",
    "            saver.restore(sess, path)\n",
    "            print'Test image acc = ', acc.eval(session=sess, feed_dict={X:test_images, Y:test_labels, TRAIN:False})\n",
    "        \n",
    "    else:\n",
    "        test_image = scipy.misc.imread(sys.argv[2])\n",
    "        test_image = test_image.reshape([1, row*col])\n",
    "        test_image = test_image / 255.0\n",
    "        with tf.Session() as sess:\n",
    "            saver.restore(sess, path)\n",
    "            np_Y_pred_softmax = sess.run(Y_pred_softmax, feed_dict={X:test_image, TRAIN:False})\n",
    "            \n",
    "            for i in range(typesOfNumber):\n",
    "                if np_Y_pred_softmax[0][i] == np_Y_pred_softmax.max():\n",
    "                    print i\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
